import whisper
import torch
import sounddevice as sd
from scipy.io.wavfile import write
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import edge_tts
import asyncio

# ===== æ¨¡å‹è·¯å¾„ =====
BASE_MODEL = "./qwen2.5-1.5b"
LORA_MODEL = "./lora-model"

# ===== åŠ è½½æ¨¡å‹ =====
asr_model = whisper.load_model("base")

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, trust_remote_code=True)
model = PeftModel.from_pretrained(model, LORA_MODEL)
model.eval()

# ===== å½•éŸ³ =====
def record_audio(filename="input.wav", duration=5, fs=16000):
    print("ğŸ™ å¼€å§‹è¯´è¯...")
    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1)
    sd.wait()
    write(filename, fs, audio)
    print("âœ… å½•éŸ³å®Œæˆ")

# ===== è¯­éŸ³è½¬æ–‡å­— =====
def speech_to_text():
    result = asr_model.transcribe("input.wav")
    print("ğŸ“ è¯†åˆ«ç»“æœ:", result["text"])
    return result["text"]

# ===== å¤§æ¨¡å‹æ¨ç† =====
def llm_reply(text):
    prompt = f"ç”¨æˆ·ï¼š{text}\nåŠ©æ‰‹ï¼š"
    inputs = tokenizer(prompt, return_tensors="pt")
    with torch.no_grad():
        output = model.generate(**inputs, max_new_tokens=200)
    reply = tokenizer.decode(output[0], skip_special_tokens=True)
    print("ğŸ¤– æ¨¡å‹å›å¤:", reply)
    return reply

# ===== æ–‡æœ¬è½¬è¯­éŸ³ =====
async def text_to_speech(text):
    communicate = edge_tts.Communicate(text=text, voice="zh-CN-XiaoxiaoNeural")
    await communicate.save("reply.mp3")
    print("ğŸ”Š å·²ç”Ÿæˆè¯­éŸ³ reply.mp3")

# ===== ä¸»æµç¨‹ =====
def main():
    record_audio()
    text = speech_to_text()
    reply = llm_reply(text)
    asyncio.run(text_to_speech(reply))

if __name__ == "__main__":
    main()
